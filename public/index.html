<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Deepfake Awareness</title>
    <link rel="stylesheet" href="css/style.css">
</head>
<body>
    <header>
        <h1>Deepfake Awareness</h1>
        <nav>
            <ul>
                <li><a href="#" onclick="showModule('introduction')">Introduction</a></li>
                <li><a href="#" onclick="showModule('case-studies')">Case Studies</a></li>
                <li><a href="#" onclick="showModule('community-forum')">Community Forum</a></li>
                <li><a href="#" onclick="showModule('flashcard-quiz')">Flashcard Quiz</a></li>
                <li><a href="#" onclick="showModule('Real-World-Discussion')">Real World Discussion</a></li>
                <li><a href="#" onclick="showModule('about')">About & Contact</a></li>
            </ul>
        </nav>
    </header>

    <main id="content">
        <!-- Introduction Module -->
        <section id="introduction" class="module">
            <h2>Overview and Origin:</h2>
            <p class="text-block"> Traditionally, deepfake refers to an image or recording that has been convincingly altered and manipulated to misrepresent someone as doing or saying something that was not actually done or said. Nowadays, with the rapid development of artificial intelligence, deepfake technology can create new images and videos that never existed.</p>

            <h2> Societal Impact: </h2>
            <p class="text-block">  The societal impact of AI deepfake technology is deeply concerning. Deepfakes can be used to create false information that damages the identity and reputation of individuals, often leading to serious consequences. For those without strong technical knowledge, these fabricated videos or images can appear convincingly real, making them easy targets for deception. This can result in devastating outcomes. I have heard that criminals produce fake videos to convince elderly people that their grandchildren have been kidnapped, or that scammers generate fake websites and travel photos to pose as a legitimate travel agency. These are clear examples of illegal misuse of the technology with harmful societal effects. Even more troubling is the issue of privacy and human rights violations. The creation of deepfakes often relies on training data composed of real photos of the people being impersonated, raising serious concerns about portrait rights and consent. Overall, while the technology itself is powerful, its unethical use poses significant risks to individuals and society as a whole.</p>
            <h2> Introduction Video </h2>
            <div class="video-container">
                <iframe
                      width="560"
                      height="315"
                      src="https://www.youtube.com/embed/YFT64MVhUZA?si=9qIc50KHOwegFsSs"
                      title="YouTube video player"
                      frameborder="0"
                      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                      allowfullscreen>
                </iframe>
            </div>
        </section>

        <!-- Case Studies Module -->
        <section id="case-studies" class="module" style="display:none;">
            <h2>Famouse Case Studies</h2>
            <p>Explore real-world examples of deepfake harassment, blackmail, and misinformation.</p>
            <article>
                <h3>Case 1: Moon Landing Deepfake</h3>
                <p> One notable case study involving deepfake technology is the "In Event of Moon Disaster" project, created by MIT's Center for Advanced Virtuality in 2020. This project reimagines a historical moment using modern AI tools—it presents a deepfaked video of U.S. President Richard Nixon delivering a contingency speech written in case the 1969 Apollo 11 Moon landing had failed. While the Moon landing was successful in reality, the speech was a real draft prepared by the Nixon administration. By using AI-generated visuals and voice cloning, the creators produced a highly realistic video of Nixon announcing the astronauts' tragic deaths. However, the purpose of this project was not to deceive but to educate. It served as a powerful demonstration of how deepfake technology can convincingly rewrite historical events and spread misinformation, especially when viewers are unaware of the manipulation. This case highlights the potential of deepfakes to distort truth and stresses the importance of digital literacy and critical thinking in the age of synthetic media. </p>
                <div class="video-container">
                    <iframe
                      width="560"
                      height="315"
                      src="https://www.youtube.com/embed/kc_ufCSQLwI?si=p4I15Zcxmk9I7o7s"
                      title="YouTube video player"
                      frameborder="0"
                      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                      allowfullscreen>
                    </iframe>
                </div>
            </article>
            <hr>
            <article>
                <h3>Case 2: Deepfake “Elon Musk”: The Internet's Biggest Scammer </h3>
                <p> Another high-profile case of deepfake abuse involves AI-generated videos of Elon Musk being used in widespread online scams. In August 2024, The New York Times labeled the deepfake “Musk” as “the Internet’s biggest scammer” due to the sheer scale of fraud tied to these deceptive videos. 
                    One particularly tragic case involved Steve Beauchamp, an 82-year-old retiree who lost $690,000 of his retirement savings after being convinced by a deepfake video that Musk was promoting a legitimate investment. Similarly, Heidi Swan told CBS News she had sent $10,000 to scammers after seeing deepfake ads of Musk on Facebook and TikTok, which convincingly mimicked his voice and appearance. Even after learning the videos were fake, she admitted they still looked and sounded just like him. Importantly, this is just one of many cases involving deepfake Elon Musk scams circulating online. The misuse of his likeness has become a recurring tool for fraud, making it a prime example of how deepfake technology can be weaponized for large-scale deception and financial harm.
                </p>
                <div class="video-container">
                    <iframe
                      width="560"
                      height="315"
                      src="https://www.youtube.com/embed/Xe1_L-a-nSI?si=gSG-EHyEPLigucTV"
                      title="YouTube video player"
                      frameborder="0"
                      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                      allowfullscreen>
                    </iframe>
                </div>
            </article>
            <hr>
            <article>
                <h3>Case 3: Joe Biden Voice </h3>
                <p> In early 2024, just before the New Hampshire Democratic primary, some voters received a robocall featuring what sounded like President Joe Biden telling them not to vote. But it wasn’t really him—it was a deepfake created using AI to mimic his voice. The message falsely claimed that Democrats should “save” their vote for the general election, aiming to confuse and discourage people from participating in the primary. This incident is a clear example of how deepfake technology can be used to spread misinformation and interfere with elections. By making it seem like trusted public figures are saying things they never actually said, deepfakes can manipulate public opinion and undermine democratic processes. The robocall raised widespread concern and highlighted the urgent need for public awareness around how easily voices and images can be faked. It also showed just how 
                    important it is to double-check unexpected messages—especially when they involve something as important as your right to vote.</p>
                <div class="video-container">
                    <iframe
                      width="560"
                      height="315"
                      src="https://www.youtube.com/embed/FCs_zFbkf0M?si=X6ezDQI6X76FBas0"
                      title="YouTube video player"
                      frameborder="0"
                      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                      allowfullscreen>
                    </iframe>
                </div>
            </article>
            <hr>
            <article>
                <h3>Case 4: Taylor swift </h3>
                <p> In January 2024, explicit AI-generated images of pop star Taylor Swift were widely shared on social media, falsely portraying her in compromising situations. Created using deepfake technology, the images were entirely fake but looked realistic enough to mislead millions of viewers before platforms began removing them. This incident highlights how deepfakes can be used to violate a person’s privacy and dignity, especially when the target is a public figure. It also drew attention to the growing problem of non-consensual deepfake pornography, a form of abuse that is illegal in many countries and deeply harmful to its victims. The case sparked public outrage and renewed calls for stronger protections and regulations, 
                    reminding us that these tools, when misused, can cause serious emotional, reputational, and legal consequences.</p>
                <div class="video-container">
                    <iframe
                          width="560"
                          height="315"
                          src="https://www.youtube.com/embed/O_BG4dPZ5A0?si=1aTKNKpblRNeGED3"
                          title="YouTube video player"
                          frameborder="0"
                          allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                          allowfullscreen>
                    </iframe>
                </div>
            </article>
        </section>

        <!-- Community Forum Module -->
        <section id="community-forum" class="module" style="display:none;">
            <h2>Community Forum</h2>
            <p>Judgement for deepfake</p>
            
            <div class="upload-container">
                <input type="file" id="image-upload" accept="image/*" multiple />
                <button id="upload-button">Upload To the Website</button>
            </div>
            
            <div id="uploaded-images-container"></div>
        </section>


        <!-- Quiz/Game Module -->
        <section id="flashcard-quiz" class="module" style="display:none;">
            <h2>Deepfake Detection Flashcard</h2>
            <h3>All the questions and answers are just insights from the developer, feel free to share your perspective with us! ❤️❤️❤️</h3>
            <div id="flashcards-container"></div>
          </section>

        <!-- Victim Support Guide Module -->
        <section id="Real-World-Discussion" class="module" style="display:none;">
            <h1>🛑 The Real-World Harm of Deepfake Technology</h1>
            <p>Deepfakes aren't just a tech buzzword—they can cause serious damage to real people in everyday life. While celebrity cases grab headlines, the truth is that anyone can be a victim, especially people with little awareness of AI or technology.</p>
                <p class="text-block">Below are realistic examples and cases that we think should take serious consideration on how deepfakes can affect vulnerable groups like elders, children, and ordinary internet users.
                </p>
            <h2>🧓 Scams Targeting the Elderly</h2>
            <p><em>📞 “It sounded just like my grandson… I sent them $5,000.” </em></p>
            <p>There have been real cases where elderly people were tricked into thinking their grandchild was kidnapped or in trouble—simply because the voice on the phone “sounded just like them.” Some were manipulated into wiring large sums of money, thinking they were helping family. I personally have the experience of an elder member in my family believing in a health product whose certificates, credentials, and endorsement from experts were all deepfaked. Normally, elderly people have little to no technological background, especially those who live in rural areas, mountains, and places that are not connected with the outside world. This makes them vulnerable targets for these scams.</p>
            <p>Insights:</p>
            <ul style="margin-left: 40px;">
                <li>AI voices used in phone scams</li>
                <li>Fake shopping sites with deepfaked reviews or celebrity endorsements</li>
                <li>Loss of money, trust, and emotional harm</li>
            </ul>
            <hr>
            <h2>🧒 Kids Misusing Deepfakes</h2>
            <p><em>😢 “Someone posted a video of me saying something I never said.”</em></p>
            <p>Deepfake tools are too accessible, and you do not need to be a Neural Network expert to generate stuff using AI. The next generation got exposed to technology way earlier than our generation. I have seen kids still in baby strollers holding an iPad when going to a restaurant. Because of the early exposure and easy access, some young people may use it to bully classmates, spread fake rumors, or embarrass others online. Imagine a child’s face being placed in a fake video or image and shared around school or social media. It can deeply affect their mental health and social relationships. Worse, they may not even understand the harm they’re causing—because it feels like a joke, but the impact is very real.</p>
            <p>Insights:</p>
            <ul style="margin-left: 40px;">
                <li>Used for bullying, harassment, and “jokes”</li>
                <li>Can damage friendships, reputations, and mental health</li>
                <li>Children may not understand the seriousness of what they're doing</li>
            </ul>
            <hr>
            <h1>The bigger picture on ordinary people</h1>
            <p>Although elders and kids are vulnerable victims, ordinary people are also targets. Nowadays, technology has allowed scammers to customize their scams for you based on your information. They can get their hands on the immense amount of your data collected by applications and websites. When you agree to those terms and conditions, there is a risk that your data has been collected. We are not focusing on data privacy, but based on these data, scammers can make their case most expertly and realistically. For example, the scammers know you are working at a certain company and you are working on some project. The scammers can then pretend to be your colleague, group leader, or even your boss. They can ask for money, threaten you, or instruct you to do something that can be both harmful to you and your company.</p>
            <p>Reflecting on my own experience, I, as an international student, got scammed years ago. The scammers claimed to be a customs officer and said a package of mine had been detained at customs due to something, and asked me for my ID and personal information. They made the case so real that the address mentioned was a real address to me, and the courier company was the one I used most often. So, as someone who has a technological background and anti-fraud awareness (I thought), I became a victim.</p>
            <hr>
            <h1>✅ What Can We Do?</h1>
            <ul style="margin-left: 40px;">
                <li>Learn how deepfakes work and how to spot them</li>
                <li>Talk to vulnerable people (elders, kids) about the risks</li>
                <li>Report fake content when you see it</li>
                <li>Support policies and platforms that fight deepfake abuse</li>
            </ul>
            <hr>
            <p class="center-bold-box">
                Deepfake abuse is not just a tech issue—it’s a human issue. The more we talk about it, the more we protect each other.
            </p>
        </section>

        <!-- About & Contact Module -->
        <section id="about" class="module" style="display:none;">
            <h2>About the Developers</h2>
          
            <div class="developer-info">
              <div class="developer">
                <h3>Ke (Wade) Yang</h3>
                <p>4th-year Computer Science student with a focus on AI & Vision. Responsible for front-end design, interactive modules, and brain-storming ideas.</p>
              </div>
          
              <div class="developer">
                <h3>Shuhao (Hilbert) Yang</h3>
                <p>4th-year Computer Science student with interets in Neural Networks, software and system security. Responsible for research cases, website design, content writing, and brain-storming ideas.</p>
              </div>
            </div>
          
            <hr>
          
            <h2>Final Remarks</h2>
            <p>
                This website was created as part of our University of Waterloo CS492 Group Project, intended solely for educational purposes and not for further development beyond the course. Our goal is to raise public awareness about the real-world harms of deepfake technology and its impact on everyday life.
                Our scope is not to provide technical deep-dives or target computer science students—but to speak to the greater public, especially those without a strong background in AI or digital technology. Our focus is on the social implications of deepfake technology. In today’s society, while general literacy continues to rise, technological literacy remains uneven, leaving many people vulnerable to digital manipulation and deception.
                If this website helps someone with limited technical knowledge better understand the risks of deepfakes and feel confident enough to talk about it or warn others, then we consider that a success.               
            </p>
            <p class="center-bold-box">You don’t need to be a tech expert to stay informed. Awareness is your first defense—share it. </p>
            <hr>
          
            <h2>References</h2>
            <ul class="indented-list">
              <li>Learning, M. O. (n.d.). Tackling the misinformation epidemic with “in event of Moon disaster.” MIT News | Massachusetts Institute of Technology. https://news.mit.edu/2020/mit-tackles-misinformation-in-event-of-moon-disaster-0720 </li>
              <li>Incode. (2025, March 5). Top 5 cases of AI deepfake fraud from 2024 exposed: Blog. https://incode.com/blog/top-5-cases-of-ai-deepfake-fraud-from-2024-exposed/ </li>
              <li>Guardian News and Media. (2024, January 22). Democrats sound alarm over AI robocall to voters mimicking Biden. The Guardian. https://www.theguardian.com/us-news/2024/jan/22/biden-fake-robocalls-new-hampshire </li>
              <li>Rahman-Jones, I. (2024, January 27). Taylor Swift Deepfakes spark calls in Congress for new legislation. BBC News. https://www.bbc.com/news/technology-68110476 </li>
              <li>BBC News. (2024, Jan 7). <em>Taylor Swift deepfakes spark calls for new legislation</em> [Video]. YouTube. https://www.youtube.com/watch?v=O_BG4dPZ5A0</li>
              <li>World Things (2023, January 3). <em>Elon Musk's Deep Fake Video Promoting a Crypto Scam</em> [Video]. YouTube. https://www.youtube.com/watch?v=Xe1_L-a-nSI&t=13s</li>
              <li>MIT Open Learning (2020, July 19). <em>In Event Of Moon Disaster Movie Trailer [Video]</em> YouTube. https://www.youtube.com/watch?v=kc_ufCSQLwI&t=43s</li>
            </ul>
          </section>
          
    </main>

    <footer>
        <p>&copy; 2025 Deepfake Awareness Project</p>
    </footer>
    <script src="js/script.js"></script>
</body>
</html>
